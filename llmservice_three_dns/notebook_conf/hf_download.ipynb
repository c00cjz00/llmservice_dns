{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b2f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 套件安裝\n",
    "!pip install opencc opencc-python-reimplemented datasets ipywidgets IProgress  openpyxl pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2597027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'factory_saves': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# 連線資料\n",
    "!rm factory_data\n",
    "!rm factory_saves\n",
    "!ln -s /storage/factory_data .\n",
    "!ln -s /storage/factory_saves ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3812de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型下載\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id = \"INX-TEXT/Bailong-instruct-7B\" # hugginFace's model name\n",
    "snapshot_download(\n",
    "    repo_id=model_id, \n",
    "    local_dir=\"INX-TEXT_Bailong-instruct-7B\",\n",
    "    local_dir_use_symlinks=False,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=\"<YOUR_HF_ACCESS_TOKEN>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd81015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP資料製作\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# HF資料集 Example01\n",
    "dataset_dict = load_dataset(\n",
    "    \"AWeirdDev/zh-tw-articles-2k\",\n",
    "    cache_dir=\"cache\",  # 方便清理\n",
    "    streaming=True,  # 啟用此選項，避免整份資料集被下載到硬碟裡面\n",
    ")\n",
    "dataset = dataset_dict['train']\n",
    "\n",
    "# 轉檔\n",
    "extracted_dataset = []\n",
    "for _, data in zip(range(1000), dataset):\n",
    "#for data in dataset:\n",
    "    extracted_data = {\n",
    "        \"text\":  data[\"content\"]      \n",
    "    }\n",
    "\n",
    "    extracted_dataset.append(extracted_data)\n",
    "\n",
    "# 內容寫進檔案\n",
    "with open(\"data_cp.json\", \"wt\", encoding=\"UTF-8\") as fp:\n",
    "    json.dump(extracted_dataset, fp, ensure_ascii=False, indent=4) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "25e6d731",
   "metadata": {},
   "source": [
    "# 1: move \"data_cp.json\" to \"/storage/factory_data/dataset_info.json\" \n",
    "# 2: add content to \"/storage/factory_data/dataset_info.json\" \n",
    "  \"demo_cp\": {\n",
    "    \"file_name\": \"demo_cp.json\",\n",
    "    \"file_sha1\": \"a1bcc8956707684662fe1fc85d080b937c7019ca\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"  \n",
    "    }\n",
    "  },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9087f5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 2.37k/2.37k [00:00<00:00, 14.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "# FT 資料製作\n",
    "import json\n",
    "import opencc\n",
    "from datasets import load_dataset\n",
    "# s2t: 簡體到正體, s2twp:簡體到台灣正體\n",
    "op_cc=opencc.OpenCC('s2twp')\n",
    "\n",
    "# HF資料集 Example02\n",
    "dataset_dict = load_dataset(\n",
    "    \"ticoAg/Chinese-medical-dialogue\",\n",
    "    cache_dir=\"cache\",  # 方便清理\n",
    "    streaming=True,  # 啟用此選項，避免整份資料集被下載到硬碟裡面\n",
    ")\n",
    "dataset = dataset_dict['train']\n",
    "\n",
    "# 轉檔\n",
    "extracted_dataset = []\n",
    "for _, data in zip(range(1000), dataset):\n",
    "#for data in dataset:\n",
    "    extracted_data = {\n",
    "        \"instruction\":  op_cc.convert(data[\"instruction\"]),    \n",
    "        \"input\":  op_cc.convert(data[\"input\"]),\n",
    "        \"output\":  op_cc.convert(data[\"output\"]),        \n",
    "        \"system\":  \"You are a helpful AI assistant built by NCHC. The user you are helping speaks Traditional Chinese and comes from Taiwan.\"        \n",
    "    }\n",
    "\n",
    "    extracted_dataset.append(extracted_data)\n",
    "\n",
    "# 內容寫進檔案\n",
    "with open(\"data_ft.json\", \"wt\", encoding=\"UTF-8\") as fp:\n",
    "    json.dump(extracted_dataset, fp, ensure_ascii=False, indent=4) \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5518b03",
   "metadata": {},
   "source": [
    "# 1: move \"data_sft.json\" to \"/storage/factory_data/dataset_info.json\" \n",
    "# 2: add content to \"/storage/factory_data/dataset_info.json\" \n",
    "  \"demo_sft\": {\n",
    "    \"file_name\": \"demo_sft.json\",\n",
    "    \"file_sha1\": \"a1bcc8956707684662fe1fc85d080b937c7019ca\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"query\": \"input\",\n",
    "      \"response\": \"output\",\n",
    "      \"system\": \"system\"          \n",
    "    }\n",
    "  },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adba4b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 512/512 [00:00<00:00, 3.98MB/s]\n"
     ]
    }
   ],
   "source": [
    "# RLHF\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# s2t: 簡體到正體, s2twp:簡體到台灣正體\n",
    "#op_cc=opencc.OpenCC('s2twp')\n",
    "\n",
    "# HF資料集 Example03\n",
    "dataset_dict = load_dataset(\n",
    "    \"LawChat-tw/RLHF_data\",\n",
    "    cache_dir=\"cache\",  # 方便清理\n",
    "    streaming=True,  # 啟用此選項，避免整份資料集被下載到硬碟裡面\n",
    ")\n",
    "dataset = dataset_dict['train']\n",
    "\n",
    "# 轉檔\n",
    "extracted_dataset = []\n",
    "for _, data in zip(range(1000), dataset):\n",
    "#for data in dataset:\n",
    "    extracted_data = {\n",
    "        \"question\": data[\"prompt\"],\n",
    "        \"answer\": [data[\"chosen\"],data[\"rejected\"]],\n",
    "        \"system\": \"You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\"\n",
    "    }\n",
    "\n",
    "    extracted_dataset.append(extracted_data)\n",
    "\n",
    "# 內容寫進檔案\n",
    "with open(\"data_rlhf.json\", \"wt\", encoding=\"UTF-8\") as fp:\n",
    "    json.dump(extracted_dataset, fp, ensure_ascii=False, indent=4) \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46f5f64e",
   "metadata": {},
   "source": [
    "# 1: move \"data_sft.json\" to \"/storage/factory_data/dataset_info.json\" \n",
    "# 2: add content to \"/storage/factory_data/dataset_info.json\" \n",
    "  \"demo_rlhf\": {\n",
    "    \"file_name\": \"demo_rlhf.json\",\n",
    "    \"file_sha1\": \"a1bcc8956707684662fe1fc85d080b937c7019ca\",\n",
    "    \"ranking\": true,\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"question\", \n",
    "      \"response\": \"answer\", \n",
    "      \"system\": \"system\"\n",
    "    }\n",
    "  },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc1b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel\n",
    "import json\n",
    "import openpyxl\n",
    "\n",
    "# Excel資料集\n",
    "input_file=\"data/14-SFT_MedQA2019.xlsx\"\n",
    "wb = openpyxl.load_workbook(input_file)\n",
    "sheet = wb[\"DrugQA\"]\n",
    "\n",
    "# 轉檔\n",
    "extracted_dataset = []\n",
    "for row in sheet.iter_rows(min_row=2, max_col=4, values_only=True):\n",
    "    extracted_data = {\n",
    "        \"instruction\": row[0],\n",
    "        \"input\":  \"\",\n",
    "        \"output\": row[3],      \n",
    "        \"system\":  \"You are a helpful AI assistant built by NCHC. The user you are helping speaks Traditional Chinese and comes from Taiwan.\"        \n",
    "    }    \n",
    "    \n",
    "    extracted_dataset.append(extracted_data)\n",
    "\n",
    "# 內容寫進檔案\n",
    "with open(\"data_excel.json\", \"wt\", encoding=\"UTF-8\") as fp:\n",
    "    json.dump(extracted_dataset, fp, ensure_ascii=False, indent=4) \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d07d814e",
   "metadata": {},
   "source": [
    "  \"data_excel\": {\n",
    "    \"file_name\": \"data_excel.json\",\n",
    "    \"file_sha1\": \"a1bcc8956707684662fe1fc85d080b937c7019ca\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"query\": \"input\",\n",
    "      \"response\": \"output\",\n",
    "      \"system\": \"system\"          \n",
    "    }\n",
    "  },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "835e6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# JSON資料集\n",
    "input_file=\"data/15-SFT_baike.json\"\n",
    "data = pd.read_json ( input_file )\n",
    "df = pd.DataFrame(data) # 轉成 DataFrame\n",
    "\n",
    "# 轉檔\n",
    "extracted_dataset = []\n",
    "for index, row in df.iterrows():\n",
    "    extracted_data = {\n",
    "        \"instruction\": row['instruction'],\n",
    "        \"input\":  row['input'],\n",
    "        \"output\": row['output'],\n",
    "        \"system\":  \"You are a helpful AI assistant built by NCHC. The user you are helping speaks Traditional Chinese and comes from Taiwan.\"                \n",
    "    }\n",
    "    \n",
    "    extracted_dataset.append(extracted_data)\n",
    "\n",
    "# 內容寫進檔案\n",
    "with open(\"data_json.json\", \"wt\", encoding=\"UTF-8\") as fp:\n",
    "    json.dump(extracted_dataset, fp, ensure_ascii=False, indent=4) \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87457cfb",
   "metadata": {},
   "source": [
    "  \"data_json\": {\n",
    "    \"file_name\": \"data_json.json\",\n",
    "    \"file_sha1\": \"a1bcc8956707684662fe1fc85d080b937c7019ca\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"query\": \"input\",\n",
    "      \"response\": \"output\",\n",
    "      \"system\": \"system\"          \n",
    "    }\n",
    "  },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55b00cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity json\n",
    "import opencc\n",
    "import pandas as pd\n",
    "import json\n",
    "# s2t: 簡體到正體, s2twp:簡體到台灣正體\n",
    "op_cc=opencc.OpenCC('s2twp')\n",
    "\n",
    "## 取代字元\n",
    "NAME = \"c00cjz00\"\n",
    "AUTHOR = \"國網中心\"\n",
    "with open(\"data/16-SFT_identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "  dataset = json.load(f)\n",
    "\n",
    "for sample in dataset:\n",
    "  sample[\"output\"] = sample[\"output\"].replace(\"NAME\", NAME).replace(\"AUTHOR\", AUTHOR)\n",
    "\n",
    "with open(\"identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# JSON資料集\n",
    "input_file=\"identity.json\"\n",
    "data = pd.read_json ( input_file )\n",
    "df = pd.DataFrame(data) # 轉成 DataFrame\n",
    "\n",
    "# 轉檔\n",
    "extracted_dataset = []\n",
    "for index, row in df.iterrows():\n",
    "    instruction = op_cc.convert(row['instruction'])\n",
    "    input = op_cc.convert(row['input'])     \n",
    "    output = op_cc.convert(row['output'])     \n",
    "    extracted_data = {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\":  input,\n",
    "        \"output\": output\n",
    "    }\n",
    "    #for num in range(1, 10):\n",
    "    extracted_dataset.append(extracted_data)\n",
    "\n",
    "# 內容寫進檔案\n",
    "with open(\"data_identity.json\", \"wt\", encoding=\"UTF-8\") as fp:\n",
    "    json.dump(extracted_dataset, fp, ensure_ascii=False, indent=4) \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5729e81f",
   "metadata": {},
   "source": [
    "  \"data_identity\": {\n",
    "    \"file_name\": \"data_identity.json\",\n",
    "    \"file_sha1\": \"a1bcc8956707684662fe1fc85d080b937c7019ca\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"query\": \"input\",\n",
    "      \"response\": \"output\"\n",
    "    }\n",
    "  },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0530b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# JSON資料集\n",
    "input_file=\"data/17-SFT_train.parquet\"\n",
    "data = pd.read_parquet(input_file, engine='pyarrow')\n",
    "\n",
    "df = pd.DataFrame(data) # 轉成 DataFrame\n",
    "\n",
    "# 轉檔\n",
    "extracted_dataset = []\n",
    "for index, row in df.iterrows():\n",
    "    extracted_data = {\n",
    "        \"instruction\": row['prompt'],\n",
    "        \"input\": \"\",\n",
    "        \"output\": row['response'],\n",
    "        \"system\":  \"You are a helpful AI assistant built by NCHC. The user you are helping speaks Traditional Chinese and comes from Taiwan.\"                \n",
    "    }\n",
    "    \n",
    "    extracted_dataset.append(extracted_data)\n",
    "\n",
    "# 內容寫進檔案\n",
    "with open(\"data_parquet.json\", \"wt\", encoding=\"UTF-8\") as fp:\n",
    "    json.dump(extracted_dataset, fp, ensure_ascii=False, indent=4) \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "539142ca",
   "metadata": {},
   "source": [
    "  \"data_parquet\": {\n",
    "    \"file_name\": \"data_parquet.json\",\n",
    "    \"file_sha1\": \"a1bcc8956707684662fe1fc85d080b937c7019ca\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"query\": \"input\",\n",
    "      \"response\": \"output\",\n",
    "      \"system\": \"system\"          \n",
    "    }\n",
    "  },"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce198ba8",
   "metadata": {},
   "source": [
    "{\n",
    "  \"身份認證\": {\n",
    "    \"file_name\": \"identity.json\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"query\": \"input\",\n",
    "      \"response\": \"output\"\n",
    "    }\n",
    "  },\n",
    "  \"alpaca_en_demo\": {\n",
    "    \"file_name\": \"alpaca_en_demo.json\"\n",
    "  }, \n",
    " \"alpaca_en\": {\n",
    "    \"hf_hub_url\": \"llamafactory/alpaca_en\"\n",
    "  },\n",
    "  \"alpaca_gpt4_en\": {\n",
    "    \"hf_hub_url\": \"llamafactory/alpaca_gpt4_en\"\n",
    "  },    \n",
    "  \"taiwan_e_hospital\": {\n",
    "    \"hf_hub_url\": \"c00cjz00/taiwan_e_hospital\"\n",
    "  },       \n",
    "  \"b8.3patch1\": {\n",
    "    \"hf_hub_url\": \"c00cjz00/b8.3-patch1\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\",\n",
    "      \"system_tag\": \"system\"  \n",
    "    }\n",
    "  },\n",
    "  \"b8.3patch2\": {\n",
    "    \"hf_hub_url\": \"c00cjz00/b8.3patch2\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\",\n",
    "      \"system_tag\": \"system\"  \n",
    "    }\n",
    "  },\n",
    "  \"taiwan_song\": {\n",
    "    \"hf_hub_url\": \"c00cjz00/ft-taiwan-song\",\n",
    "    \"formatting\": \"sharegpt\",\n",
    "    \"columns\": {\n",
    "      \"messages\": \"messages\"\n",
    "    },\n",
    "    \"tags\": {\n",
    "      \"role_tag\": \"role\",\n",
    "      \"content_tag\": \"content\",\n",
    "      \"user_tag\": \"user\",\n",
    "      \"assistant_tag\": \"assistant\"\n",
    "    }\n",
    "  },\n",
    "  \"c4_demo\": {\n",
    "    \"file_name\": \"c4_demo.json\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"text\"\n",
    "    }\n",
    "  }    \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
