{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3470a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b07171",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/Xenova/llama-3-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb444ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import create_pretrained_tokenizer, create_tokenizer, encode, decode\n",
    "import json\n",
    "with open(\"llama-3-tokenizer/tokenizer.json\") as f:\n",
    "    json_data = json.load(f)\n",
    "json_str = json.dumps(json_data)\n",
    "custom_tokenizer = create_tokenizer(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import encode, decode\n",
    "sample_text = \"今天要吃麥當勞\"\n",
    "# openai encoding + decoding\n",
    "token_counter = encode(model='', custom_tokenizer=custom_tokenizer, text=sample_text)\n",
    "print(token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c723479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import encode, decode\n",
    "\n",
    "sample_text = \"麥當勞好吃\"\n",
    "# openai encoding + decoding\n",
    "openai_tokens = encode(model=\"gpt-3.5-turbo\", text=sample_text)\n",
    "print(openai_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef727eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_text = decode(model=\"gpt-4-vision-preview\", tokens=openai_tokens)\n",
    "print(openai_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import token_counter\n",
    "\n",
    "messages = [{\"user\": \"role\", \"content\": \"Hellö World, this is my input string!\"}]\n",
    "print(token_counter(model=\"gpt-3.5-turbo\", messages=messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22bdbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import token_counter\n",
    "\n",
    "messages = [{\"user\": \"role\", \"content\": \"興天簪\"}]\n",
    "print(token_counter(model=\"gpt-4.\", messages=messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d0c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import create_pretrained_tokenizer, create_tokenizer\n",
    "\n",
    "# get tokenizer from huggingface repo\n",
    "custom_tokenizer_1 = create_pretrained_tokenizer(\"Xenova/llama-3-tokenizer\")\n",
    "\n",
    "# use tokenizer from json file\n",
    "with open(\"tokenizer.json\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "json_str = json.dumps(json_data)\n",
    "\n",
    "custom_tokenizer_2 = create_tokenizer(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13edd79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm \n",
    "from litellm import completion\n",
    "import os \n",
    "\n",
    "# set env variables\n",
    "os.environ[\"LITELLM_TOKEN\"] = \"9e45fa33-7e53-414f-8f37-64cae9b71d34\" # your unique litellm token\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"openai key\"\n",
    "\n",
    "litellm.use_client = True # enable logging dashboard \n",
    "messages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
    "\n",
    "# openai call\n",
    "#response = completion(model=\"gpt-3.5-turbo\", messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a900de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter(messages=messages, model=\"Xenova/llama-3-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af03e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d95c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import encode, decode\n",
    "\n",
    "sample_text = \"Hello, how are you?\"\n",
    "# openai encoding + decoding\n",
    "openai_tokens = encode(model=\"clarifai/meta.Llama-2.llama2-7b-chat\", text=sample_text)\n",
    "print(openai_tokens)\n",
    "\n",
    "#openai_text = decode(model=\"clarifai/meta.Llama-2.llama2-7b-chat\", tokens=openai_tokens)\n",
    "#print(openai_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{ \"content\": \"麥當勞?\",\"role\": \"user\"}]\n",
    "token_counter(messages=messages, model=\"clarifai/meta.Llama-2.llama2-7b-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307672c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import token_counter\n",
    "\n",
    "model = 'gpt-3.5-turbo'  # \n",
    "model = 'gpt-4-1106-preview'\n",
    "text = '今天藍'\n",
    "\n",
    "count = token_counter(model=model, text=text, count_response_tokens=True)\n",
    "print(token_counter(model=\"gpt-3.5-turbo\", text=text))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9850c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '今天藍'\n",
    "llama2_tokens = token_counter(model=\"meta-llama/Llama-2\", text=sample_text)\n",
    "llama2_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c222bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import create_pretrained_tokenizer, create_tokenizer, encode, decode, token_counter\n",
    "import json\n",
    "with open(\"taide/Llama-2-7b/tokenizer.json\") as f:\n",
    "    json_data = json.load(f)\n",
    "json_str = json.dumps(json_data)\n",
    "custom_tokenizer = create_tokenizer(json_str)\n",
    "\n",
    "### test the openai, claude, cohere and llama2 tokenizers.\n",
    "### The tokenizer value should be different for all\n",
    "sample_text = \"今天要吃麥當勞\"\n",
    "\n",
    "# openai tokenizer\n",
    "openai_tokens = token_counter(model=\"gpt-3.5-turbo\", text=sample_text)\n",
    "\n",
    "# llama2 tokenizer\n",
    "llama2_tokens = token_counter(model=\"meta-llama/Llama-2-7b\", text=sample_text)\n",
    "\n",
    "# llama2 tokenizer\n",
    "llama3_tokens = token_counter(model=\"meta-llama/Llama-3-8b\", text=sample_text)\n",
    "\n",
    "# 自訂 tokenizer\n",
    "my_tokens = token_counter(model='', custom_tokenizer=custom_tokenizer, text=sample_text)\n",
    "\n",
    "print(\n",
    "    f\"openai tokens: {openai_tokens}; llama2 tokens: {llama2_tokens}; llama3 tokens: {llama3_tokens}\"\n",
    ")\n",
    "print(\n",
    "    f\"my tokens: {my_tokens}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c159132",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/taide/Llama3-TAIDE-LX-8B-Chat-Alpha1/raw/main/tokenizer.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff79612",
   "metadata": {},
   "outputs": [],
   "source": [
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    tokens = token_counter(model=\"gpt-4-vision-preview\", messages=messages)\n",
    "    print(f\"tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### test the openai, claude, cohere and llama2 tokenizers.\n",
    "### The tokenizer value should be different for all\n",
    "sample_text = \"Hellö World, this is my input string! My name is ishaan CTO\"\n",
    "\n",
    "# openai tokenizer\n",
    "openai_tokens = token_counter(model=\"gpt-3.5-turbo\", text=sample_text)\n",
    "\n",
    "# claude tokenizer\n",
    "claude_tokens = token_counter(model=\"claude-instant-1\", text=sample_text)\n",
    "\n",
    "# cohere tokenizer\n",
    "cohere_tokens = token_counter(model=\"command-nightly\", text=sample_text)\n",
    "\n",
    "# llama2 tokenizer\n",
    "llama2_tokens = token_counter(\n",
    "    model=\"meta-llama/Llama-2-7b-chat\", text=sample_text\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"openai tokens: {openai_tokens}; claude tokens: {claude_tokens}; cohere tokens: {cohere_tokens}; llama2 tokens: {llama2_tokens}\"\n",
    ")\n",
    "\n",
    "# assert that all token values are different\n",
    "assert (\n",
    "    openai_tokens != cohere_tokens != llama2_tokens\n",
    "), \"Token values are not different.\"\n",
    "\n",
    "print(\"test tokenizer: It worked!\")\n",
    "    except Exception as e:\n",
    "pytest.fail(f\"An exception occured: {e}\")\n",
    "\n",
    "\n",
    "# test_tokenizers()\n",
    "\n",
    "\n",
    "def test_encoding_and_decoding():\n",
    "    try:\n",
    "sample_text = \"Hellö World, this is my input string!\"\n",
    "# openai encoding + decoding\n",
    "openai_tokens = encode(model=\"gpt-3.5-turbo\", text=sample_text)\n",
    "openai_text = decode(model=\"gpt-3.5-turbo\", tokens=openai_tokens)\n",
    "\n",
    "assert openai_text == sample_text\n",
    "\n",
    "# claude encoding + decoding\n",
    "claude_tokens = encode(model=\"claude-instant-1\", text=sample_text)\n",
    "claude_text = decode(model=\"claude-instant-1\", tokens=claude_tokens.ids)\n",
    "\n",
    "assert claude_text == sample_text\n",
    "\n",
    "# cohere encoding + decoding\n",
    "cohere_tokens = encode(model=\"command-nightly\", text=sample_text)\n",
    "cohere_text = decode(model=\"command-nightly\", tokens=cohere_tokens.ids)\n",
    "\n",
    "assert cohere_text == sample_text\n",
    "\n",
    "# llama2 encoding + decoding\n",
    "llama2_tokens = encode(model=\"meta-llama/Llama-2-7b-chat\", text=sample_text)\n",
    "llama2_text = decode(\n",
    "    model=\"meta-llama/Llama-2-7b-chat\", tokens=llama2_tokens.ids\n",
    ")\n",
    "\n",
    "assert llama2_text == sample_text\n",
    "    except Exception as e:\n",
    "pytest.fail(f\"An exception occured: {e}\")\n",
    "\n",
    "\n",
    "# test_encoding_and_decoding()\n",
    "\n",
    "\n",
    "def test_gpt_vision_token_counting():\n",
    "    messages = [\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "{\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "{\n",
    "    \"type\": \"image_url\",\n",
    "    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "},\n",
    "    ],\n",
    "}\n",
    "    ]\n",
    "    tokens = token_counter(model=\"gpt-4-vision-preview\", messages=messages)\n",
    "    print(f\"tokens: {tokens}\")\n",
    "\n",
    "\n",
    "# test_gpt_vision_token_counting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f714d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
